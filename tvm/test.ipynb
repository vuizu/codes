{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tir:\n",
      " #[version = \"0.0.5\"]\n",
      "@mmult = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"mmult\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], []),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  for (x: int32, 0, 1024) {\n",
      "    for (y: int32, 0, 1024) {\n",
      "      C_3: Buffer(C_2, float32, [1048576], [])[((x*1024) + y)] = 0f32\n",
      "      for (k: int32, 0, 1024) {\n",
      "        let cse_var_2: int32 = (x*1024)\n",
      "        let cse_var_1: int32 = (cse_var_2 + y)\n",
      "        C_3[cse_var_1] = (C_3[cse_var_1] + (A_3: Buffer(A_2, float32, [1048576], [])[(cse_var_2 + k)]*B_3: Buffer(B_2, float32, [1048576], [])[((k*1024) + y)]))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "/* For debugging purposes the metadata section has been omitted.\n",
      " * If you would like to see the full metadata section you can set the \n",
      " * option to `True` when invoking `astext`. \n",
      " */\n",
      "source code:\n",
      " // tvm target: c -keys=cpu \n",
      "#define TVM_EXPORTS\n",
      "#include \"tvm/runtime/c_runtime_api.h\"\n",
      "#include \"tvm/runtime/c_backend_api.h\"\n",
      "#include <math.h>\n",
      "#include <stdbool.h>\n",
      "#ifdef __cplusplus\n",
      "extern \"C\"\n",
      "#endif\n",
      "TVM_DLL int32_t mmult(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n",
      "#ifdef __cplusplus\n",
      "extern \"C\"\n",
      "#endif\n",
      "TVM_DLL int32_t mmult(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n",
      "  int32_t A_code = arg_type_ids[0];\n",
      "  int32_t B_code = arg_type_ids[1];\n",
      "  int32_t C_code = arg_type_ids[2];\n",
      "  void* A = (((TVMValue*)args)[0].v_handle);\n",
      "  void* B = (((TVMValue*)args)[1].v_handle);\n",
      "  void* C = (((TVMValue*)args)[2].v_handle);\n",
      "  void* mmult_A_shape = (((DLTensor*)A)[0].shape);\n",
      "  void* mmult_A_strides = (((DLTensor*)A)[0].strides);\n",
      "  int32_t dev_id = (((DLTensor*)A)[0].device.device_id);\n",
      "  void* A_1 = (((DLTensor*)A)[0].data);\n",
      "  void* mmult_B_shape = (((DLTensor*)B)[0].shape);\n",
      "  void* mmult_B_strides = (((DLTensor*)B)[0].strides);\n",
      "  void* B_1 = (((DLTensor*)B)[0].data);\n",
      "  void* mmult_C_shape = (((DLTensor*)C)[0].shape);\n",
      "  void* mmult_C_strides = (((DLTensor*)C)[0].strides);\n",
      "  void* C_1 = (((DLTensor*)C)[0].data);\n",
      "  if (!(mmult_A_strides == NULL)) {\n",
      "  }\n",
      "  if (!(mmult_B_strides == NULL)) {\n",
      "  }\n",
      "  if (!(mmult_C_strides == NULL)) {\n",
      "  }\n",
      "  for (int32_t x = 0; x < 1024; ++x) {\n",
      "    for (int32_t y = 0; y < 1024; ++y) {\n",
      "      ((float*)C_1)[((x * 1024) + y)] = 0.000000e+00f;\n",
      "      for (int32_t k = 0; k < 1024; ++k) {\n",
      "        int32_t cse_var_2 = (x * 1024);\n",
      "        int32_t cse_var_1 = (cse_var_2 + y);\n",
      "        ((float*)C_1)[cse_var_1] = (((float*)C_1)[cse_var_1] + (((float*)A_1)[(cse_var_2 + k)] * ((float*)B_1)[((k * 1024) + y)]));\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "\n",
      "// CodegenC: NOTE: Auto-generated entry function\n",
      "#ifdef __cplusplus\n",
      "extern \"C\"\n",
      "#endif\n",
      "TVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n",
      "  return mmult(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm import te\n",
    "\n",
    "M = 1024\n",
    "K = 1024\n",
    "N = 1024\n",
    "\n",
    "k = te.reduce_axis((0, K), 'k')\n",
    "A = te.placeholder((M, K), name='A')\n",
    "B = te.placeholder((K, N), name='B')\n",
    "C = te.compute(\n",
    "    (M, N),\n",
    "    lambda x, y: te.sum(A[x, k] * B[k, y], axis=k),\n",
    "    name='C')\n",
    "\n",
    "s = te.create_schedule(C.op)\n",
    "ir_m = tvm.lower(s, [A, B, C], simple_mode=True, name='mmult')\n",
    "rt_m = tvm.build(ir_m, [A, B, C], target='c', name='mmult')\n",
    "\n",
    "print(\"tir:\\n\", ir_m.astext(show_meta_data=False))\n",
    "\n",
    "print(\"source code:\\n\", rt_m.get_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "11.8\n",
      "True\n",
      "NVIDIA GeForce RTX 2060 with Max-Q Design\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def print_torch_version():\n",
    "    print(torch.__version__)\n",
    "    print(torch.version.cuda)\n",
    "    print(torch.cuda.is_available())\n",
    "    print(torch.cuda.get_device_name())\n",
    "    print(torch.cuda.device_count())\n",
    "\n",
    "print_torch_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
